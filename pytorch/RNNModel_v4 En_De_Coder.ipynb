{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "当输入序列长度和输出标签序列长度都可变时(即输入序列的子序列个数和输出序列的子序列个数不同)\n",
    "\n",
    "数据预处理:\n",
    "指定子序列长度,填充<pad>词元或裁剪得到固定长度的子序列,增加<eos>词元至子序列末尾\n",
    "\n",
    "encoder(编码器):\n",
    "1.子序列(batchsize,num_steps,vocabsize)用embeding层降维至(batchsize,num_steps,embed_size) #num_steps是子序列的词元个数,即RNN的时间步骤个数\n",
    "2.子序列(batchsize,num_steps,vocabsize)输出为(batchsize,num_steps,num_hiddens)和每个子序列的末尾时间步骤的状态(batchsize,num_layers,num_hiddens) #num_hidddens是隐藏层特征数,num_layers是RNN层数\n",
    "\n",
    "decoder(编码器):(N个长度固定的子序列->batchsize个为一批次,子序列长度为num_steps)\n",
    "1.子序列(batchsize,num_steps,vocabsize)用embeding层降维至(batchsize,num_steps,embed_size) #num_steps是子序列的词元个数,即RNN的时间步骤个数\n",
    "2.取encoder输出的最后一个时间步骤的预测的特征(batchsize,num_hiddens),添加为decoder子序列的特征(batchsize,num_steps,embed_size+num_hiddens) #decoder的每个时间步都包含encoder的特征?\n",
    "3.拼接后的decoder子序列(batchsize,num_steps,embed_size+num_hiddens)的RNN输出为(batchsize,num_steps,num_hiddens),并通过linear层转为(batchsize,num_steps,vocabsize),其中RNN初始状态是encoder的末尾时间步骤的状态(batchsize,num_layers,num_hiddens) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zncyxiong\\AppData\\Local\\anaconda3\\envs\\pytorch_python3128\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lit_encoder(pl.lightning.LightningModule):\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0):\n",
    "        super(Lit_encoder,self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn = torch.nn.GRU(embed_size,num_hiddens,num_layers,batch_first=True,dropout=dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        outputs,state = self.rnn(x)\n",
    "        #outputs: (batch_size,num_steps,num_hiddens)\n",
    "        #state: (batch_size,num_layers,num_hiddens)\n",
    "        return outputs, state\n",
    "\n",
    "class Lit_decoder(pl.lightening.LightningModule):\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0):\n",
    "        super(Lit_decoder,self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn = torch.nn.GRU(embed_size+num_hiddens,num_hiddens,num_layers,batch_first=True,dropout=dropout)\n",
    "        self.dense = torch.nn.Linear(num_hiddens,vocab_size)\n",
    "    \n",
    "    def forward(self,x,state):\n",
    "        x = self.embedding(x)\n",
    "        enc_outputs, enc_state = state\n",
    "        context=enc_outputs[-1]\n",
    "        context=context.repeat(x.shape[1],1,1)\n",
    "        x=torch.cat((x,context),2)\n",
    "        outputs,dec_state = self.rnn(x,enc_state)\n",
    "        outputs=self.dense(outputs)\n",
    "        return outputs, [enc_outputs,dec_state]\n",
    "\n",
    "class Lit_encoder_decoder(pl.lightning.LightningModule):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Lit_encoder_decoder,self).__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "\n",
    "    def forward(self,enc_x,dec_x):\n",
    "        enc_result=self.encoder(enc_x)\n",
    "        dec_result=self.decoder(dec_x,enc_result)\n",
    "        return dec_result[0]\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y=batch\n",
    "        y_pred,_=self(x,y) #\n",
    "        loss=torch.nn.functional.cross_entropy(y_pred.view(-1,y_pred.shape[-1]),y.view(-1))\n",
    "        self.log('train_loss',loss, prog_bar=True, logger=True, on_epoch=True,on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "class LitLoadData_fra(pl.LightningDataModule):  \n",
    "    def prepare_data(self):\n",
    "        url = 'http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip'\n",
    "        #文件是否存在\n",
    "        if os.path.exists('./data/fra-eng/fra.txt'):\n",
    "            return\n",
    "        #下载文件\n",
    "        r = requests.get(url, stream=True)\n",
    "        #解压文件\n",
    "        with zipfile.ZipFile('./data/fra-eng.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('./data/fra-eng')\n",
    "            \n",
    "data=LitLoadData_fra()\n",
    "data.prepare_data()\n",
    "#返回fra.txt内容\n",
    "with open('./data/fra-eng/fra.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_txt = f.read()\n",
    "print(raw_txt[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['hi', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['run', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['run', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['who', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['wow', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "[['<bos>', 'va', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'salut', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'cours', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'courez', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'qui', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'ça', 'alors', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(raw_txt,max_tokens=10000,num_steps=9):\n",
    "    #大写字母改为小写\n",
    "    raw_txt=raw_txt.lower()\n",
    "    #去掉空行,取前max_tokens行\n",
    "    lines=raw_txt.split('\\n')\n",
    "    lines=[line for line in lines if len(line)>0]\n",
    "    lines=lines[:max_tokens]\n",
    "    #每行以 tab 分割为两组\n",
    "    pairs=[line.split('\\t') for line in lines]\n",
    "    #删除空行\n",
    "    pairs=[pair for pair in pairs if len(pair)==2]\n",
    "    #每组单词分割,标点符号视为一个独立单词\n",
    "    pairs=[[re.findall(r'\\w+|[^\\w\\s]',pair[0]),re.findall(r'\\w+|[^\\w\\s]',pair[1])] for pair in pairs]\n",
    "    #返回源语言和目标语言\n",
    "    src=[pair[0] for pair in pairs]\n",
    "    tgt=[pair[1] for pair in pairs]\n",
    "    #末尾添加特殊字符'<eos>'\n",
    "    src=[pair+['<eos>'] for pair in src]\n",
    "    tgt=[pair+['<eos>'] for pair in tgt]\n",
    "    #tgt前面添加特殊字符'<bos>'\n",
    "    tgt=[['<bos>']+pair for pair in tgt]\n",
    "    #裁剪或填充'<pad>'至num_steps\n",
    "    src=[pair[:num_steps]+['<pad>']*(num_steps-len(pair)) if len(pair)<num_steps else pair[:num_steps] for pair in src]\n",
    "    tgt=[pair[:num_steps]+['<pad>']*(num_steps-len(pair)) if len(pair)<num_steps else pair[:num_steps] for pair in tgt]\n",
    "    return src,tgt\n",
    "\n",
    "\n",
    "LitLoadData_fra.preprocess=preprocess \n",
    "\n",
    "src,tgt=preprocess(raw_txt)\n",
    "print(src[:6])\n",
    "print(tgt[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def vocab(sentences,min_freq=0):\n",
    "    tokens=[token for sentence in sentences for token in sentence]\n",
    "    counter=collections.Counter(tokens)\n",
    "    #去掉频率小于min_freq的单词\n",
    "    tokens=[token for token in counter if counter[token]>=min_freq]\n",
    "    #token的idx按频率降序\n",
    "    tokens=sorted(tokens,key=lambda x:counter[x],reverse=True)\n",
    "    idx_to_token=[['<unk>']+token for token in tokens]\n",
    "    token_to_idx={token:idx for idx,token in enumerate(idx_to_token)}\n",
    "    return idx_to_token,token_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用tokenizers训练文本自动提取token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_text: Go.\n",
      "Encoded: ['<bos>', 'Go', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Decoded: Go .\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import BertPreTokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "class TokenizerTrainer:\n",
    "    def __init__(self, num_step=9):\n",
    "        self.num_step = num_step\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.pre_tokenizer = BertPreTokenizer()\n",
    "        self.tokenizer.add_special_tokens([\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "        self.trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"], min_frequency=2)\n",
    "        self.tokenizer.enable_padding(pad_id=self.tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\", length=self.num_step)\n",
    "        self.tokenizer.enable_truncation(max_length=self.num_step)\n",
    "        self.tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"<bos> $A <eos>\",\n",
    "            pair=\"<bos> $A <eos> <bos> $B <eos>\",\n",
    "            special_tokens=[\n",
    "                (\"<bos>\", self.tokenizer.token_to_id(\"<bos>\")),\n",
    "                (\"<eos>\", self.tokenizer.token_to_id(\"<eos>\")),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def train(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        pairs = [line.split('\\t') for line in lines]\n",
    "        self.tokenizer.train_from_iterator(pairs, self.trainer)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.tokenizer.save(path)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode(ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_python3128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
