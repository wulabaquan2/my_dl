{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "LSTM层: 在RNN上增加4个单元:3个控制单元+1个记忆单元,实现对状态H管控\n",
    "### 记忆单元:C(memory cell)\n",
    "用于控制状态H的记忆C,通过更新记忆C来控制状态H\n",
    "计算方法: 结果有用tanh转为[-1,1]\n",
    "    C=tanh(X*W_xc+H_pre*W_hc+b_c)\n",
    "### 控制单元\n",
    "输入门,输出门,遗忘门\n",
    "计算方法相同:结果用sigma转为[0,1]\n",
    "    I=sig(X*W_xi+H_pre*W_hi+b_i)\n",
    "    F=sig(X*W_xf+H_pre*W_hf+b_f)\n",
    "    O=sig(X*W_xo+H_pre*W_ho+b_o)\n",
    "#### input gate(输入门:I)\n",
    "控制当前输入对当前记忆的影响, 比如当前输入是冗余信息时当前记忆不受当前输入影响,生成的当前状态保留了更多历史状态的信息\n",
    "#### output gate(输出门:O)\n",
    "控制记忆对状态H的影响, 比如当前记忆有时效性不应该对当前状态有影响 \n",
    "#### forget gate(遗忘门:F)  \n",
    "控制历史记忆对当前记忆的影响, 比如历史记忆无参考价值\n",
    "### 工作原理\n",
    "1. 用当前输入X和历史状态H_pre得到F,I,C_1,O\n",
    "2. C_2=tanh(F.*C_2_pre+I.*C_1) #当前原生记忆用输入格式化,历史加工记忆用遗忘格式化,两者相加得到当前加工记忆\n",
    "3. H=O.*C_2 #当前加工记忆用输出格式化后生成当前状态\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义\n",
    "LSTM层原始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class Lit_LSTMModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, nums_hidden, nums_layers, lr,sigma=0.01):\n",
    "        super(Lit_LSTMModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        #input gate, forget gate, cell gate, output gate\n",
    "        self.w_xi=torch.nn.Parameter(torch.randn(vocab_size, nums_hidden)*sigma)\n",
    "        self.w_hi=torch.nn.Parameter(torch.randn(nums_hidden, nums_hidden)*sigma)\n",
    "        self.b_i=torch.nn.Parameter(torch.zeros(nums_hidden))\n",
    "        self.w_xf=torch.nn.Parameter(torch.randn(vocab_size, nums_hidden)*sigma)\n",
    "        self.w_hf=torch.nn.Parameter(torch.randn(nums_hidden, nums_hidden)*sigma)\n",
    "        self.b_f=torch.nn.Parameter(torch.zeros(nums_hidden))\n",
    "        self.w_xc=torch.nn.Parameter(torch.randn(vocab_size, nums_hidden)*sigma)\n",
    "        self.w_hc=torch.nn.Parameter(torch.randn(nums_hidden, nums_hidden)*sigma)\n",
    "        self.b_c=torch.nn.Parameter(torch.zeros(nums_hidden))\n",
    "        self.w_xo=torch.nn.Parameter(torch.randn(vocab_size, nums_hidden)*sigma)\n",
    "        self.w_ho=torch.nn.Parameter(torch.randn(nums_hidden, nums_hidden)*sigma)\n",
    "        self.b_o=torch.nn.Parameter(torch.zeros(nums_hidden))\n",
    "        #y\n",
    "        self.w_hy=torch.nn.Parameter(torch.randn(nums_hidden, vocab_size)*sigma)\n",
    "        self.b_y=torch.nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def forward(self, x, h_c=None):\n",
    "        if h_c is None:\n",
    "            h=torch.zeros(x.size(0), self.hparams.nums_hidden,device=x.device)\n",
    "            c=torch.zeros(x.size(0), self.hparams.nums_hidden,device=x.device)\n",
    "        else:\n",
    "            h,c=h_c\n",
    "        x=torch.nn.functional.one_hot(x, num_classes=self.hparams.vocab_size).float()\n",
    "        output=[]\n",
    "        for i in range(x.size(1)):\n",
    "            i_gate=torch.sigmoid(x[:,i,:]@self.w_xi+self.b_i+h@self.w_hi)\n",
    "            f_gate=torch.sigmoid(x[:,i,:]@self.w_xf+self.b_f+h@self.w_hf)\n",
    "            c_gate=torch.tanh(x[:,i,:]@self.w_xc+self.b_c+h@self.w_hc)\n",
    "            c=f_gate*c+i_gate*c_gate\n",
    "            o_gate=torch.sigmoid(x[:,i,:]@self.w_xo+self.b_o+h@self.w_ho)\n",
    "            h=o_gate*torch.tanh(c)\n",
    "            output.append(h@self.w_hy+self.b_y)\n",
    "        return torch.stack(output, dim=1), (h,c)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred,_ = self(x) #y_pred.shape: (batch_size, seq_len, output_size)\n",
    "        loss= torch.nn.functional.cross_entropy(y_pred.view(-1, y_pred.size(-1)), y.view(-1)) #输入是(batchsize* seq_len, vocab_size)和(batcsize* seq_len),拉平子序列计算单字符损失\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True, on_epoch=True,on_step=True) \n",
    "        #perplexeity用于评估大段文本的好坏,单字符loss不适合评估大段文本\n",
    "        self.log('train_perplexity', torch.exp(loss), prog_bar=True, logger=True, on_epoch=True,on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred,_ = self(x)\n",
    "        loss= torch.nn.functional.cross_entropy(y_pred.view(-1, y_pred.size(-1)), y.view(-1))\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True,on_epoch=True)\n",
    "        self.log('val_perplexity', torch.exp(loss), prog_bar=True, logger=True,on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.hparams.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义(api)\n",
    "和torch.RNN相同LSTM也因为有num_layers参数,h需要转为3D:(num_layers,batch_size,hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LstmModel_api(Lit_LSTMModel):\n",
    "    def __init__(self, vocab_size, nums_hidden, nums_layers, lr,sigma=0.01):\n",
    "        super(LstmModel_api, self).__init__(vocab_size, nums_hidden, nums_layers, lr,sigma)\n",
    "        self.lstm=nn.LSTM(vocab_size, nums_hidden, nums_layers, batch_first=True)\n",
    "        self.fc=nn.Linear(nums_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h_c=None):\n",
    "        if h_c is None:\n",
    "            h_c=(torch.zeros(self.hparams.nums_layers, x.size(0), self.hparams.nums_hidden,device=x.device),\n",
    "                  torch.zeros(self.hparams.nums_layers, x.size(0), self.hparams.nums_hidden,device=x.device))\n",
    "        x=torch.nn.functional.one_hot(x, num_classes=self.hparams.vocab_size).float()\n",
    "        x,h_c=self.lstm(x,h_c)\n",
    "        x=self.fc(x)\n",
    "        return x,h_c\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "class LitLoadData_timeMachine(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32,seq_length=5,pin_memory=True,nums_train=10000,nums_val=5000):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.pin_memory = pin_memory\n",
    "        self.nums_train = nums_train\n",
    "        self.nums_val = nums_val\n",
    "        self.prepare_data()\n",
    "        self.corpus_indices, self.char_to_idx, self.idx_to_char, self.vocab_size = self.load_data_time_machine()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        url = 'http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'\n",
    "        #文件是否存在\n",
    "        if os.path.exists('../data/timemachine.txt'):\n",
    "            return\n",
    "        #下载文件\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open('../data/timemachine.txt', 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    def load_data_time_machine(self):\n",
    "        with open('../data/timemachine.txt') as f:\n",
    "            corpus_chars = f.read()\n",
    "        #非字母替换为空格,并转为小写\n",
    "        corpus_chars = re.sub('[^A-Za-z]+', ' ', corpus_chars).lower()\n",
    "        #corpus_chars统计字符集,共26个字母+1个空格\n",
    "        char_set=set(corpus_chars) \n",
    "        #增加'<unknown>'字符,防止用户输入非上述字母内容\n",
    "        char_set.add('<unknown>')\n",
    "        #索引到字符的映射\n",
    "        idx_to_char = list(char_set) \n",
    "        #字符到索引的映射\n",
    "        char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "        vocab_size = len(char_to_idx)  #28个字符\n",
    "        corpus_indices = [char_to_idx[char] for char in corpus_chars] # 将每个字符转化为索引\n",
    "        return corpus_indices, char_to_idx, idx_to_char, vocab_size #返回索引列表,字符到索引的映射,索引到字符的映射,字典大小\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.corpus_indices, self.char_to_idx, self.idx_to_char, self.vocab_size = self.load_data_time_machine()\n",
    "        #self.corpus_indices = torch.tensor(self.corpus_indices) \n",
    "        #self.train_indices = self.corpus_indices[0: int(len(self.corpus_indices) * 0.8)] #前80%作为训练集\n",
    "        #self.valid_indices = self.corpus_indices[int(len(self.corpus_indices) * 0.8):] #后20%作为验证集\n",
    "        \n",
    "        #d2l: step=1提取子序列,子序列个数=字符总数-子序列长度; 常规方法是等分,子序列个数=字符总数/子序列长度\n",
    "        array=torch.tensor([self.corpus_indices[i:i+self.seq_length+1] for i in range(len(self.corpus_indices)-self.seq_length)])\n",
    "        self.train_indices = array[0: self.nums_train] \n",
    "        self.valid_indices = array[self.nums_train: self.nums_train + self.nums_val]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = self.__dateset_d2l(self.train_indices)\n",
    "        return torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=self.pin_memory)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = self.__dateset_d2l(self.valid_indices)\n",
    "        return torch.utils.data.DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=self.pin_memory)\n",
    "\n",
    "    #子序列个数=N,输入取1:N-1,输出取2:N\n",
    "    def __dateset_d2l(self, data_indices):\n",
    "        return torch.utils.data.TensorDataset(data_indices[:, :-1], data_indices[:, 1:])\n",
    "\n",
    "    #用于创建数据集对象。它根据序列长度将数据索引分割成多个样本，并将每个样本的输入和目标数据分别返回\n",
    "    def __dataset(self, data_indices):\n",
    "        num_samples = (len(data_indices) - 1) // self.seq_length #样本个数\n",
    "        data_indices = data_indices[:num_samples * self.seq_length] #只取前num_samples * self.seq_length个字符\n",
    "        data_indices = data_indices.reshape((num_samples, self.seq_length)) \n",
    "        return torch.utils.data.TensorDataset(data_indices[:, :-1], data_indices[:, 1:]) #每个样本的输入是前seq_length-1个字符,输出是后seq_length-1个字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工作流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\zncyxiong\\AppData\\Local\\anaconda3\\envs\\pytorch_python3128\\Lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "c:\\Users\\zncyxiong\\AppData\\Local\\anaconda3\\envs\\pytorch_python3128\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory D:\\algorithm\\deeplearning_zh.d2l.ai\\pytorch\\checkPoint-logs\\RNNModel_v2 exists and is not empty.\n",
      "\n",
      "  | Name         | Type   | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | lstm         | LSTM   | 7.9 K  | train\n",
      "1 | fc           | Linear | 924    | train\n",
      "  | other params | n/a    | 8.7 K  | n/a  \n",
      "------------------------------------------------\n",
      "17.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.6 K    Total params\n",
      "0.070     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zncyxiong\\AppData\\Local\\anaconda3\\envs\\pytorch_python3128\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zncyxiong\\AppData\\Local\\anaconda3\\envs\\pytorch_python3128\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\zncyxiong\\AppData\\Local\\anaconda3\\envs\\pytorch_python3128\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s, v_num=7, train_loss_step=1.490, train_perplexity_step=4.420, val_loss=1.870, val_perplexity=6.540, train_loss_epoch=1.470, train_perplexity_epoch=4.350]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s, v_num=7, train_loss_step=1.490, train_perplexity_step=4.420, val_loss=1.870, val_perplexity=6.540, train_loss_epoch=1.470, train_perplexity_epoch=4.350]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_module = LitLoadData_timeMachine(batch_size=1024,seq_length=32,pin_memory=False)\n",
    "    data_module.setup()\n",
    "\n",
    "    ##############RNN模型训练################\n",
    "    model=Lit_LSTMModel(\n",
    "        vocab_size=data_module.vocab_size,\n",
    "        nums_hidden=32,\n",
    "        nums_layers=1,\n",
    "        lr=4\n",
    "    )\n",
    "    model_api=LstmModel_api(\n",
    "        vocab_size=data_module.vocab_size,\n",
    "        nums_hidden=32,\n",
    "        nums_layers=1,\n",
    "        lr=4\n",
    "    )\n",
    "\n",
    "    checkpoint_callback=pl.callbacks.ModelCheckpoint(\n",
    "        monitor='val_perplexity',\n",
    "        dirpath='checkPoint-logs/RNNModel_v2',\n",
    "        filename='RNNModel_v2_{epoch:02d}_{val_perplexity:.2f}',\n",
    "        #save_top_k=3, # save the top 3 models\n",
    "        mode='min',\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        gradient_clip_algorithm='norm', #梯度裁剪算法,等同clip_gradients(self, grad_clip_val, model)\n",
    "        gradient_clip_val=1,\n",
    "        accelerator='cpu',\n",
    "        #devices=1,\n",
    "        logger=TensorBoardLogger('tensorBoard-logs/', name='RNNModel_v2'),\n",
    "        callbacks=[checkpoint_callback]\n",
    "                        )\n",
    "    #trainer.fit(model, data_module) \n",
    "    trainer.fit(model_api, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-851e88b4c639bd8a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-851e88b4c639bd8a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir pytorch/tensorBoard-logs/RNNModel_v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_python3128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
