{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "GRU层: LSTM层的简化版,只有两个控制单元\n",
    "与LSTM的最大区别是LSTM通过更新记忆单元来控制状态,而GRU是直接控制状态\n",
    "### reset gate(重置门:R)\n",
    "生成一个新的隐藏状态,内容是对历史状态控制影响后的状态\n",
    "计算: Hy_1=tanh(x*W_xh+(R.*H_pre)*W_hh+b_h)\n",
    "### update gate(更新门:Z)\n",
    "控制当前状态依赖历史状态多,还是隐藏状态多\n",
    "计算: H=Z.*H_pre+(1-Z).*Hy_1\n",
    "当Z都为1时全部保留历史状态; 当Z等于0时历史状态都需要控制\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义\n",
    "GRU层原始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class Lit_GRUModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, nums_hidden, nums_layers, lr,sigma=0.01):\n",
    "        super(Lit_GRUModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        #reset gate\n",
    "        self.w_xr=torch.nn.Parameter(torch.randn(vocab_size, nums_hidden)*sigma)\n",
    "        self.w_hr=torch.nn.Parameter(torch.randn(nums_hidden, nums_hidden)*sigma)\n",
    "        self.b_r=torch.nn.Parameter(torch.zeros(nums_hidden))\n",
    "        #update gate\n",
    "        self.w_xz=torch.nn.Parameter(torch.randn(vocab_size, nums_hidden)*sigma)\n",
    "        self.w_hz=torch.nn.Parameter(torch.randn(nums_hidden, nums_hidden)*sigma)\n",
    "        self.b_z=torch.nn.Parameter(torch.zeros(nums_hidden))\n",
    "        #hidden state\n",
    "        self.w_xh=torch.nn.Parameter(torch.randn(vocab_size, nums_hidden)*sigma) \n",
    "        self.w_hh=torch.nn.Parameter(torch.randn(nums_hidden, nums_hidden)*sigma)\n",
    "        self.b_h=torch.nn.Parameter(torch.zeros(nums_hidden))\n",
    "        #y\n",
    "        self.w_hy=torch.nn.Parameter(torch.randn(nums_hidden, vocab_size)*sigma)\n",
    "        self.b_y=torch.nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            h=torch.zeros(x.size(0), self.hparams.nums_hidden)\n",
    "        x=torch.nn.functional.one_hot(x, num_classes=self.hparams.vocab_size).float()\n",
    "        output=[]\n",
    "        for i in range(x.size(1)):\n",
    "            r=torch.sigmoid(x[:,i]@self.w_xr+self.b_r+h@self.w_hr)\n",
    "            z=torch.sigmoid(x[:,i]@self.w_xz+self.b_z+h@self.w_hz)\n",
    "            h_hat=torch.tanh(x[:,i]@self.w_xh+self.b_h+r*(h@self.w_hh))\n",
    "            h=z*h+(1-z)*h_hat\n",
    "            output.append(h@self.w_hy+self.b_y)\n",
    "        return torch.stack(output, dim=1), h\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred,_ = self(x) #y_pred.shape: (batch_size, seq_len, output_size)\n",
    "        loss= torch.nn.functional.cross_entropy(y_pred.view(-1, y_pred.size(-1)), y.view(-1)) #输入是(batchsize* seq_len, vocab_size)和(batcsize* seq_len),拉平子序列计算单字符损失\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True, on_epoch=True,on_step=True) \n",
    "        #perplexeity用于评估大段文本的好坏,单字符loss不适合评估大段文本\n",
    "        self.log('train_perplexity', torch.exp(loss), prog_bar=True, logger=True, on_epoch=True,on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred,_ = self(x)\n",
    "        loss= torch.nn.functional.cross_entropy(y_pred.view(-1, y_pred.size(-1)), y.view(-1))\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True,on_epoch=True)\n",
    "        self.log('val_perplexity', torch.exp(loss), prog_bar=True, logger=True,on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.hparams.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义(api)\n",
    "和torch.RNN相同GRU也因为有num_layers参数,h需要转为3D:(num_layers,batch_size,hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUModel_api(Lit_GRUModel):\n",
    "    def __init__(self, vocab_size, nums_hidden, nums_layers, lr,sigma=0.01):\n",
    "        super(GRUModel_api, self).__init__(vocab_size, nums_hidden, nums_layers, lr,sigma)\n",
    "        self.gru=nn.GRU(input_size=vocab_size, hidden_size=nums_hidden, num_layers=nums_layers, batch_first=True)\n",
    "        self.fc=nn.Linear(nums_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            h=torch.zeros(self.hparams.nums_layers, x.size(0), self.hparams.nums_hidden)\n",
    "        x=torch.nn.functional.one_hot(x, num_classes=self.hparams.vocab_size).float()\n",
    "        x,h=self.gru(x,h)\n",
    "        x=self.fc(x)\n",
    "        return x,h\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "class LitLoadData_timeMachine(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32,seq_length=5,pin_memory=True,nums_train=10000,nums_val=5000):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.pin_memory = pin_memory\n",
    "        self.nums_train = nums_train\n",
    "        self.nums_val = nums_val\n",
    "        self.prepare_data()\n",
    "        self.corpus_indices, self.char_to_idx, self.idx_to_char, self.vocab_size = self.load_data_time_machine()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        url = 'http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'\n",
    "        #文件是否存在\n",
    "        if os.path.exists('../data/timemachine.txt'):\n",
    "            return\n",
    "        #下载文件\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open('../data/timemachine.txt', 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    def load_data_time_machine(self):\n",
    "        with open('../data/timemachine.txt') as f:\n",
    "            corpus_chars = f.read()\n",
    "        #非字母替换为空格,并转为小写\n",
    "        corpus_chars = re.sub('[^A-Za-z]+', ' ', corpus_chars).lower()\n",
    "        #corpus_chars统计字符集,共26个字母+1个空格\n",
    "        char_set=set(corpus_chars) \n",
    "        #增加'<unknown>'字符,防止用户输入非上述字母内容\n",
    "        char_set.add('<unknown>')\n",
    "        #索引到字符的映射\n",
    "        idx_to_char = list(char_set) \n",
    "        #字符到索引的映射\n",
    "        char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "        vocab_size = len(char_to_idx)  #28个字符\n",
    "        corpus_indices = [char_to_idx[char] for char in corpus_chars] # 将每个字符转化为索引\n",
    "        return corpus_indices, char_to_idx, idx_to_char, vocab_size #返回索引列表,字符到索引的映射,索引到字符的映射,字典大小\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.corpus_indices, self.char_to_idx, self.idx_to_char, self.vocab_size = self.load_data_time_machine()\n",
    "        #self.corpus_indices = torch.tensor(self.corpus_indices) \n",
    "        #self.train_indices = self.corpus_indices[0: int(len(self.corpus_indices) * 0.8)] #前80%作为训练集\n",
    "        #self.valid_indices = self.corpus_indices[int(len(self.corpus_indices) * 0.8):] #后20%作为验证集\n",
    "        \n",
    "        #d2l: step=1提取子序列,子序列个数=字符总数-子序列长度; 常规方法是等分,子序列个数=字符总数/子序列长度\n",
    "        array=torch.tensor([self.corpus_indices[i:i+self.seq_length+1] for i in range(len(self.corpus_indices)-self.seq_length)])\n",
    "        self.train_indices = array[0: self.nums_train] \n",
    "        self.valid_indices = array[self.nums_train: self.nums_train + self.nums_val]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = self.__dateset_d2l(self.train_indices)\n",
    "        return torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=self.pin_memory)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = self.__dateset_d2l(self.valid_indices)\n",
    "        return torch.utils.data.DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=self.pin_memory)\n",
    "\n",
    "    #子序列个数=N,输入取1:N-1,输出取2:N\n",
    "    def __dateset_d2l(self, data_indices):\n",
    "        return torch.utils.data.TensorDataset(data_indices[:, :-1], data_indices[:, 1:])\n",
    "\n",
    "    #用于创建数据集对象。它根据序列长度将数据索引分割成多个样本，并将每个样本的输入和目标数据分别返回\n",
    "    def __dataset(self, data_indices):\n",
    "        num_samples = (len(data_indices) - 1) // self.seq_length #样本个数\n",
    "        data_indices = data_indices[:num_samples * self.seq_length] #只取前num_samples * self.seq_length个字符\n",
    "        data_indices = data_indices.reshape((num_samples, self.seq_length)) \n",
    "        return torch.utils.data.TensorDataset(data_indices[:, :-1], data_indices[:, 1:]) #每个样本的输入是前seq_length-1个字符,输出是后seq_length-1个字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工作流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 6.8 K  | n/a \n",
      "---------------------------------------------\n",
      "6.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.8 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/10 [00:00<?, ?it/s, v_num=1, train_loss_step=2.810, train_perplexity_step=16.70, val_loss=2.790, val_perplexity=16.20, train_loss_epoch=2.840, train_perplexity_epoch=17.00]         "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_module = LitLoadData_timeMachine(batch_size=1024,seq_length=32,pin_memory=False)\n",
    "    data_module.setup()\n",
    "\n",
    "    ##############RNN模型训练################\n",
    "    model=Lit_GRUModel(\n",
    "        vocab_size=data_module.vocab_size,\n",
    "        nums_hidden=32,\n",
    "        nums_layers=1,\n",
    "        lr=4\n",
    "    )\n",
    "    model_api=GRUModel_api(\n",
    "        vocab_size=data_module.vocab_size,\n",
    "        nums_hidden=32,\n",
    "        nums_layers=1,\n",
    "        lr=4\n",
    "    )\n",
    "\n",
    "    checkpoint_callback=pl.callbacks.ModelCheckpoint(\n",
    "        monitor='val_perplexity',\n",
    "        dirpath='checkPoint-logs/RNNModel_v3',\n",
    "        filename='RNNModel_v3_{epoch:02d}_{val_perplexity:.2f}',\n",
    "        #save_top_k=3, # save the top 3 models\n",
    "        mode='min',\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        gradient_clip_algorithm='norm', #梯度裁剪算法,等同clip_gradients(self, grad_clip_val, model)\n",
    "        gradient_clip_val=1,\n",
    "        accelerator='cpu',\n",
    "        #devices=1,\n",
    "        logger=TensorBoardLogger('tensorBoard-logs/', name='RNNModel_v3'),\n",
    "        callbacks=[checkpoint_callback]\n",
    "                        )\n",
    "    trainer.fit(model, data_module) \n",
    "    #trainer.fit(model_api, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-851e88b4c639bd8a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-851e88b4c639bd8a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir pytorch/tensorBoard-logs/RNNModel_v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_python3128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
