{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 官方有提供onnx导出,但只有decoder缺少encoder.\n",
    "> 结合encoder和decoder并精简input和output参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import onnx\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 添加到 sys.path\n",
    "sys.path.append(r'D:\\algorithm\\deeplearning\\Semantic Segmentation\\segment-anything-main')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from ..modeling import Sam\n",
    "from .amg import calculate_stability_score\n",
    "\n",
    "\n",
    "class SamOnnxModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model should not be called directly, but is used in ONNX export.\n",
    "    It combines the prompt encoder, mask decoder, and mask postprocessing of Sam,\n",
    "    with some functions modified to enable model tracing. Also supports extra\n",
    "    options controlling what information. See the ONNX export script for details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Sam,\n",
    "        return_single_mask: bool,\n",
    "        use_stability_score: bool = False,\n",
    "        return_extra_metrics: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mask_decoder = model.mask_decoder\n",
    "        self.model = model\n",
    "        self.img_size = model.image_encoder.img_size\n",
    "        self.return_single_mask = return_single_mask\n",
    "        self.use_stability_score = use_stability_score\n",
    "        self.stability_score_offset = 1.0\n",
    "        self.return_extra_metrics = return_extra_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_longest_image_size(\n",
    "        input_image_size: torch.Tensor, longest_side: int\n",
    "    ) -> torch.Tensor:\n",
    "        input_image_size = input_image_size.to(torch.float32)\n",
    "        scale = longest_side / torch.max(input_image_size)\n",
    "        transformed_size = scale * input_image_size\n",
    "        transformed_size = torch.floor(transformed_size + 0.5).to(torch.int64)\n",
    "        return transformed_size\n",
    "\n",
    "    def _embed_points(self, point_coords: torch.Tensor, point_labels: torch.Tensor) -> torch.Tensor:\n",
    "        point_coords = point_coords + 0.5\n",
    "        point_coords = point_coords / self.img_size\n",
    "        point_embedding = self.model.prompt_encoder.pe_layer._pe_encoding(point_coords)\n",
    "        point_labels = point_labels.unsqueeze(-1).expand_as(point_embedding)\n",
    "\n",
    "        point_embedding = point_embedding * (point_labels != -1)\n",
    "        point_embedding = point_embedding + self.model.prompt_encoder.not_a_point_embed.weight * (\n",
    "            point_labels == -1\n",
    "        )\n",
    "\n",
    "        for i in range(self.model.prompt_encoder.num_point_embeddings):\n",
    "            point_embedding = point_embedding + self.model.prompt_encoder.point_embeddings[\n",
    "                i\n",
    "            ].weight * (point_labels == i)\n",
    "\n",
    "        return point_embedding\n",
    "\n",
    "    def _embed_masks(self, input_mask: torch.Tensor, has_mask_input: torch.Tensor) -> torch.Tensor:\n",
    "        mask_embedding = has_mask_input * self.model.prompt_encoder.mask_downscaling(input_mask)\n",
    "        mask_embedding = mask_embedding + (\n",
    "            1 - has_mask_input\n",
    "        ) * self.model.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
    "        return mask_embedding\n",
    "\n",
    "    def mask_postprocessing(self, masks: torch.Tensor, orig_im_size: torch.Tensor) -> torch.Tensor:\n",
    "        masks = F.interpolate(\n",
    "            masks,\n",
    "            size=(self.img_size, self.img_size),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        prepadded_size = self.resize_longest_image_size(orig_im_size, self.img_size).to(torch.int64)\n",
    "        masks = masks[..., : prepadded_size[0], : prepadded_size[1]]  # type: ignore\n",
    "\n",
    "        orig_im_size = orig_im_size.to(torch.int64)\n",
    "        h, w = orig_im_size[0], orig_im_size[1]\n",
    "        masks = F.interpolate(masks, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
    "        return masks\n",
    "\n",
    "    def select_masks(\n",
    "        self, masks: torch.Tensor, iou_preds: torch.Tensor, num_points: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Determine if we should return the multiclick mask or not from the number of points.\n",
    "        # The reweighting is used to avoid control flow.\n",
    "        score_reweight = torch.tensor(\n",
    "            [[1000] + [0] * (self.model.mask_decoder.num_mask_tokens - 1)]\n",
    "        ).to(iou_preds.device)\n",
    "        score = iou_preds + (num_points - 2.5) * score_reweight\n",
    "        best_idx = torch.argmax(score, dim=1)\n",
    "        masks = masks[torch.arange(masks.shape[0]), best_idx, :, :].unsqueeze(1)\n",
    "        iou_preds = iou_preds[torch.arange(masks.shape[0]), best_idx].unsqueeze(1)\n",
    "\n",
    "        return masks, iou_preds\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        point_coords: torch.Tensor,\n",
    "        point_labels: torch.Tensor,\n",
    "        mask_input: torch.Tensor,\n",
    "        has_mask_input: torch.Tensor,\n",
    "        orig_im_size: torch.Tensor,\n",
    "    ):\n",
    "        sparse_embedding = self._embed_points(point_coords, point_labels)\n",
    "        dense_embedding = self._embed_masks(mask_input, has_mask_input)\n",
    "\n",
    "        masks, scores = self.model.mask_decoder.predict_masks(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embedding,\n",
    "            dense_prompt_embeddings=dense_embedding,\n",
    "        )\n",
    "\n",
    "        if self.use_stability_score:\n",
    "            scores = calculate_stability_score(\n",
    "                masks, self.model.mask_threshold, self.stability_score_offset\n",
    "            )\n",
    "\n",
    "        if self.return_single_mask:\n",
    "            masks, scores = self.select_masks(masks, scores, point_coords.shape[1])\n",
    "\n",
    "        upscaled_masks = self.mask_postprocessing(masks, orig_im_size)\n",
    "\n",
    "        if self.return_extra_metrics:\n",
    "            stability_scores = calculate_stability_score(\n",
    "                upscaled_masks, self.model.mask_threshold, self.stability_score_offset\n",
    "            )\n",
    "            areas = (upscaled_masks > self.model.mask_threshold).sum(-1).sum(-1)\n",
    "            return upscaled_masks, scores, stability_scores, areas, masks\n",
    "\n",
    "        return upscaled_masks, scores, masks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
