{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 检查huggingface是否支持Model: https://huggingface.co/docs/optimum/exporters/onnx/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optimum[exporters]\n",
    "%pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 optimum.onnxruntime 将 huggingFace Transformers 模型导出为 ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\condaEnv\\neurips\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tmp/onnx/tokenizer_config.json',\n",
       " 'tmp/onnx/special_tokens_map.json',\n",
       " 'tmp/onnx/vocab.txt',\n",
       " 'tmp/onnx/added_tokens.json',\n",
       " 'tmp/onnx/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "save_directory = \"tmp/onnx/\"\n",
    "\n",
    "# Load a model from transformers and export it to ONNX\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n",
    "\n",
    "# Save the ONNX model and tokenizer\n",
    "ort_model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [option] cpu推理加速: onnx-model转quantization-onnx-model\n",
    "量化（Quantization） 是将 浮点数（FP32） 转换为 低精度整数（如 INT8、INT16），从而减少 模型大小，提高 计算速度，特别适用于 边缘设备和服务器。\n",
    "\n",
    "动态量化（Dynamic Quantization） 的特点：\n",
    "\n",
    "- 只量化权重，激活值（activation）仍然使用浮点计算。\n",
    "- 在推理时，动态地 将 FP32 激活值转换为 INT8，然后计算，再转换回 FP32。\n",
    "- 适用于 Transformer、BERT、LSTM 等 全连接层（MatMul、GEMM）较多 的模型。\n",
    "- 优点：推理速度显著提高，精度损失小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('tmp/onnx')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "\n",
    "# Define the quantization methodology\n",
    "qconfig = AutoQuantizationConfig.avx2(is_static=False, per_channel=False) #cpu: Intel Core i7-10750H\n",
    "quantizer = ORTQuantizer.from_pretrained(ort_model)\n",
    "\n",
    "# Apply dynamic quantization on the model\n",
    "quantizer.quantize(save_dir=save_directory, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [option] 模型优化:ORTOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 现成优化参数: AutoOptimizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('tmp/onnx')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime import AutoOptimizationConfig\n",
    "\n",
    "optimization_config = AutoOptimizationConfig.O2()\n",
    "optimizer = ORTOptimizer.from_pretrained(\"tmp/onnx\") #tmp/onnx中的model.onnx\n",
    "optimizer.optimize(save_dir=\"tmp/onnx\", optimization_config=optimization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义优化参数: OptimizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\condaEnv\\neurips\\lib\\site-packages\\optimum\\onnxruntime\\configuration.py:784: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('tmp/onnx')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import  OptimizationConfig\n",
    "\n",
    "optimization_config = OptimizationConfig(\n",
    "    optimization_level=2,\n",
    "    enable_transformers_specific_optimizations=True,\n",
    "    optimize_for_gpu=False,\n",
    ")\n",
    "optimizer = ORTOptimizer.from_pretrained(\"tmp/onnx\") #tmp/onnx中的model.onnx\n",
    "optimizer.optimize(save_dir=\"tmp/onnx\",file_suffix=\"customOptimized\", optimization_config=optimization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用onnx-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用本地的onnx-model(推荐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997308850288391}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"tmp/onnx/\", file_name=\"model_quantized.onnx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tmp/onnx/\")\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "results = classifier(\"I love burritos!\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用huggingface的ortmodel\n",
    "下载官方导出的model.onnx后调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9041659235954285, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.pipelines import pipeline\n",
    "#huggingface没有distilbert-base-uncased-finetuned-sst-2-english,改为调用deepset/roberta-base-squad2的ortmodel作为示例\n",
    "onnx_qa = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", accelerator=\"ort\")\n",
    "question = \"What's my name?\"\n",
    "context = \"My name is Philipp and I live in Nuremberg.\"\n",
    "\n",
    "pred = onnx_qa(question=question, context=context)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tmp/onnx文件结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-- config.json\n",
      "|-- model.onnx\n",
      "|-- model_customOptimized.onnx\n",
      "|-- model_optimized.onnx\n",
      "|-- model_quantized.onnx\n",
      "|-- ort_config.json\n",
      "|-- special_tokens_map.json\n",
      "|-- tokenizer.json\n",
      "|-- tokenizer_config.json\n",
      "|-- vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def display_file_tree(directory, indent=0):\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        print(\"  \" * indent + \"|-- \" + item)\n",
    "        if os.path.isdir(item_path):\n",
    "            display_file_tree(item_path, indent + 1)\n",
    "\n",
    "display_file_tree(\"tmp/onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
